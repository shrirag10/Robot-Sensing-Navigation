{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea772083-e4e1-4b77-910c-a448f735b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d52543b4-4985-4c4e-9aa0-ce3a07382bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_compute_keypoints(image):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Harris Corner Detection\n",
    "    gray_float = np.float32(gray)\n",
    "    harris_response = cv2.cornerHarris(gray_float, blockSize=2, ksize=3, k=0.04)\n",
    "\n",
    "    # Dilate the corner points to mark them\n",
    "    harris_response = cv2.dilate(harris_response, None)\n",
    "    \n",
    "    # Thresholding to get only the corners\n",
    "    threshold = 0.005 * harris_response.max()\n",
    "    keypoints = np.argwhere(harris_response > threshold)\n",
    "    \n",
    "    # Check if we found any keypoints\n",
    "    if len(keypoints) == 0:\n",
    "        print(\"No keypoints detected\")\n",
    "        return [], None\n",
    "\n",
    "    # Convert keypoints to cv2.KeyPoint format\n",
    "    keypoints = [cv2.KeyPoint(float(x[1]), float(x[0]), 1) for x in keypoints]\n",
    "    \n",
    "    # Compute descriptors using ORB (this is just an example, you can use any descriptor method)\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints, descriptors = orb.compute(image, keypoints)\n",
    "\n",
    "    # If no descriptors found, return None\n",
    "    if descriptors is None:\n",
    "        print(\"No descriptors found\")\n",
    "        return [], None\n",
    "    \n",
    "    return keypoints, descriptors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aecde0b3-7628-434a-9618-748afb378fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_keypoints(descriptors1, descriptors2):\n",
    "    # Handle the case where descriptors might be None\n",
    "    if descriptors1 is None or descriptors2 is None:\n",
    "        print(\"Descriptors are None, cannot match keypoints.\")\n",
    "        return []\n",
    "    \n",
    "    # Use Brute Force Matcher with Hamming distance\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    \n",
    "    # Match descriptors between the two images\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "    # Sort matches based on distance (best match comes first)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    \n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "076cce57-327e-4b37-a887-a00ce8919ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_images(img1, img2, keypoints1, keypoints2, matches):\n",
    " \n",
    "    # Extract the matching points from the keypoints based on the matches\n",
    "    points1 = np.array([keypoints1[m.queryIdx].pt for m in matches])  # Points from img1\n",
    "    points2 = np.array([keypoints2[m.trainIdx].pt for m in matches])  # Points from img2\n",
    "    \n",
    "    # Convert points to integers\n",
    "    points1 = points1.astype(np.float32)\n",
    "    points2 = points2.astype(np.float32)\n",
    "\n",
    "    # Compute homography matrix using RANSAC to get the best possible transformation\n",
    "    H, mask = cv2.findHomography(points2, points1, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # Get the dimensions of the images\n",
    "    height1, width1 = img1.shape[:2]\n",
    "    height2, width2 = img2.shape[:2]\n",
    "\n",
    "    # Warp img2 to img1's coordinate system using the homography matrix\n",
    "    img2_warped = cv2.warpPerspective(img2, H, (width1 + width2, height1))\n",
    "\n",
    "    # Place img1 into the left part of the canvas\n",
    "    img2_warped[0:height1, 0:width1] = img1\n",
    "\n",
    "    # Create a result image (panorama)\n",
    "    panorama = img2_warped\n",
    "\n",
    "    return panorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ea622c5-6c40-4f63-8e80-a865984da1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_panorama(images):\n",
    "    # Start with the first image\n",
    "    panorama = images[0]\n",
    "    \n",
    "    for i in range(1, len(images)):\n",
    "        # Detect and compute keypoints and descriptors for each image\n",
    "        keypoints1, descriptors1 = detect_and_compute_keypoints(panorama)\n",
    "        keypoints2, descriptors2 = detect_and_compute_keypoints(images[i])\n",
    "\n",
    "        # Check if descriptors are valid and have sufficient rows for matching\n",
    "        if len(keypoints1) == 0 or len(keypoints2) == 0 or descriptors1 is None or descriptors2 is None:\n",
    "            print(f\"Skipping image {i} due to lack of sufficient features\")\n",
    "            continue  # Skip this pair and move to the next image\n",
    "\n",
    "        # Match keypoints between the current panorama and the next image\n",
    "        matches = match_keypoints(descriptors1, descriptors2)\n",
    "\n",
    "        # If there are not enough matches, skip the image\n",
    "        if len(matches) < 10:\n",
    "            print(f\"Not enough matches between images {i-1} and {i}. Skipping image {i}.\")\n",
    "            continue\n",
    "\n",
    "        # Stitch the images together\n",
    "        panorama = stitch_images(panorama, images[i], keypoints1, keypoints2, matches)\n",
    "    \n",
    "    return panorama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cddf9a4-ab5a-4b6f-9800-33e38e5006fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\bld\\libopencv_1727758951658\\work\\modules\\features2d\\src\\matchers.cpp:860: error: (-215:Assertion failed) trainDescCollection[iIdx].rows < IMGIDX_ONE in function 'cv::BFMatcher::knnMatchImpl'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m images \u001b[38;5;241m=\u001b[39m [cv2\u001b[38;5;241m.\u001b[39mimread(img_path) \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m image_paths]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Create the panorama\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m panorama \u001b[38;5;241m=\u001b[39m create_panorama(images)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Display the result\u001b[39;00m\n\u001b[0;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n",
      "Cell \u001b[1;32mIn[33], line 16\u001b[0m, in \u001b[0;36mcreate_panorama\u001b[1;34m(images)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip this pair and move to the next image\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Match keypoints between the current panorama and the next image\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m matches \u001b[38;5;241m=\u001b[39m match_keypoints(descriptors1, descriptors2)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# If there are not enough matches, skip the image\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matches) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:\n",
      "Cell \u001b[1;32mIn[31], line 11\u001b[0m, in \u001b[0;36mmatch_keypoints\u001b[1;34m(descriptors1, descriptors2)\u001b[0m\n\u001b[0;32m      8\u001b[0m bf \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mBFMatcher(cv2\u001b[38;5;241m.\u001b[39mNORM_HAMMING, crossCheck\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Match descriptors between the two images\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m matches \u001b[38;5;241m=\u001b[39m bf\u001b[38;5;241m.\u001b[39mmatch(descriptors1, descriptors2)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Sort matches based on distance (best match comes first)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(matches, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mdistance)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\bld\\libopencv_1727758951658\\work\\modules\\features2d\\src\\matchers.cpp:860: error: (-215:Assertion failed) trainDescCollection[iIdx].rows < IMGIDX_ONE in function 'cv::BFMatcher::knnMatchImpl'\n"
     ]
    }
   ],
   "source": [
    "# Load your set of images (make sure they are in the correct order)\n",
    "image_paths = [\n",
    "    \"C:\\\\Users\\\\ragsh\\\\Desktop\\\\FALL 24\\\\RSN\\\\LAB5\\\\data\\\\image_1.jpg\",\n",
    "    \"C:\\\\Users\\\\ragsh\\\\Desktop\\\\FALL 24\\\\RSN\\\\LAB5\\\\data\\\\image_2.jpg\",\n",
    "    \"C:\\\\Users\\\\ragsh\\\\Desktop\\\\FALL 24\\\\RSN\\\\LAB5\\\\data\\\\image_3.jpg\",\n",
    "    \"C:\\\\Users\\\\ragsh\\\\Desktop\\\\FALL 24\\\\RSN\\\\LAB5\\\\data\\\\image_4.jpg\",\n",
    "    \"C:\\\\Users\\\\ragsh\\\\Desktop\\\\FALL 24\\\\RSN\\\\LAB5\\\\data\\\\image_5.jpg\",\n",
    "    \"C:\\\\Users\\\\ragsh\\\\Desktop\\\\FALL 24\\\\RSN\\\\LAB5\\\\data\\\\image_6.jpg\"\n",
    "]\n",
    "images = [cv2.imread(img_path) for img_path in image_paths]\n",
    "\n",
    "# Create the panorama\n",
    "panorama = create_panorama(images)\n",
    "\n",
    "# Display the result\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(cv2.cvtColor(panorama, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3807693c-ec01-4bfa-ab4f-36cc8938b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_and_compute_keypoints(image):\n",
    "    \"\"\"\n",
    "    Detect Harris corners and compute ORB descriptors.\n",
    "    \n",
    "    Arguments:\n",
    "    image -- The input image for feature detection.\n",
    "    \n",
    "    Returns:\n",
    "    keypoints -- Detected keypoints.\n",
    "    descriptors -- Descriptors for the detected keypoints.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Harris corner detection\n",
    "    harris_response = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
    "\n",
    "    # Threshold for the Harris response\n",
    "    threshold = 0.01 * harris_response.max()\n",
    "    keypoints = np.argwhere(harris_response > threshold)\n",
    "    keypoints = [cv2.KeyPoint(x[1], x[0], 1) for x in keypoints]  # Convert to cv2.KeyPoint format\n",
    "    \n",
    "    # Compute ORB descriptors (you could use other descriptors as well)\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints, descriptors = orb.compute(gray, keypoints)\n",
    "\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def match_keypoints(descriptors1, descriptors2):\n",
    "    \"\"\"\n",
    "    Match descriptors between two sets using Brute-Force Matcher.\n",
    "    \n",
    "    Arguments:\n",
    "    descriptors1 -- Descriptors of the first image.\n",
    "    descriptors2 -- Descriptors of the second image.\n",
    "    \n",
    "    Returns:\n",
    "    matches -- List of matched keypoints.\n",
    "    \"\"\"\n",
    "    if descriptors1 is None or descriptors2 is None:\n",
    "        return []\n",
    "\n",
    "    # Use Brute Force Matcher (NORM_HAMMING is used for binary descriptors like ORB)\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    \n",
    "    # Match descriptors\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "    \n",
    "    # Sort matches based on distance (best matches first)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def stitch_images(img1, img2, keypoints1, keypoints2, matches):\n",
    "    \"\"\"\n",
    "    Stitch img1 and img2 together based on matched keypoints and homography.\n",
    "    \n",
    "    Arguments:\n",
    "    img1 -- The first image.\n",
    "    img2 -- The second image.\n",
    "    keypoints1 -- Keypoints from img1.\n",
    "    keypoints2 -- Keypoints from img2.\n",
    "    matches -- List of matched keypoints.\n",
    "    \n",
    "    Returns:\n",
    "    panorama -- Stitched panorama image.\n",
    "    \"\"\"\n",
    "    # Extract points from keypoints based on matches\n",
    "    points1 = np.array([keypoints1[m.queryIdx].pt for m in matches])\n",
    "    points2 = np.array([keypoints2[m.trainIdx].pt for m in matches])\n",
    "    \n",
    "    points1 = points1.astype(np.float32)\n",
    "    points2 = points2.astype(np.float32)\n",
    "    \n",
    "    # Compute homography matrix using RANSAC\n",
    "    H, _ = cv2.findHomography(points2, points1, cv2.RANSAC, 5.0)\n",
    "    \n",
    "    # Get the dimensions of the images\n",
    "    height1, width1 = img1.shape[:2]\n",
    "    height2, width2 = img2.shape[:2]\n",
    "    \n",
    "    # Warp img2 to the perspective of img1\n",
    "    img2_warped = cv2.warpPerspective(img2, H, (width1 + width2, height1))\n",
    "    \n",
    "    # Place img1 into the left part of the result panorama\n",
    "    img2_warped[0:height1, 0:width1] = img1\n",
    "    \n",
    "    return img2_warped\n",
    "\n",
    "def create_panorama(images):\n",
    "    \"\"\"\n",
    "    Create a panorama by stitching multiple images together.\n",
    "    \n",
    "    Arguments:\n",
    "    images -- List of images to stitch.\n",
    "    \n",
    "    Returns:\n",
    "    panorama -- Final stitched panorama.\n",
    "    \"\"\"\n",
    "    panorama = images[0]\n",
    "    \n",
    "    for i in range(1, len(images)):\n",
    "        # Detect and compute keypoints and descriptors for each image\n",
    "        keypoints1, descriptors1 = detect_and_compute_keypoints(panorama)\n",
    "        keypoints2, descriptors2 = detect_and_compute_keypoints(images[i])\n",
    "        \n",
    "        # Check if descriptors are valid\n",
    "        if len(keypoints1) == 0 or len(keypoints2) == 0 or descriptors1 is None or descriptors2 is None:\n",
    "            print(f\"Skipping image {i} due to lack of sufficient features\")\n",
    "            continue\n",
    "        \n",
    "        # Match keypoints between the two images\n",
    "        matches = match_keypoints(descriptors1, descriptors2)\n",
    "        \n",
    "        # If there are not enough matches, skip this pair\n",
    "        if len(matches) < 10:\n",
    "            print(f\"Not enough matches between images {i-1} and {i}. Skipping image {i}.\")\n",
    "            continue\n",
    "        \n",
    "        # Stitch the images together\n",
    "        panorama = stitch_images(panorama, images[i], keypoints1, keypoints2, matches)\n",
    "    \n",
    "    return panorama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd77d7-4fdf-40a9-9afb-803cc3a1bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your set of images (make sure they are in the correct order)\n",
    "image_paths = [\n",
    "    \"C:\\\\Users\\\\ragsh\\\\Desktop\\\\FALL 24\\\\RSN\\\\LAB5\\\\data\\\\image_1.jpg\",\n",
    "    \"C:\\\\Users\\\\ragsh\\\\Desktop\\\\FALL 24\\\\RSN\\\\LAB5\\\\data\\\\image_2.jpg\",\n",
    "    \"C:\\\\Users\\\\ragsh\\\\Desktop\\\\FALL 24\\\\RSN\\\\LAB5\\\\data\\\\image_3.jpg\",\n",
    "    \"C:\\\\Users\\\\ragsh\\\\Desktop\\\\FALL 24\\\\RSN\\\\LAB5\\\\data\\\\image_4.jpg\",\n",
    "    \"C:\\\\Users\\\\ragsh\\\\Desktop\\\\FALL 24\\\\RSN\\\\LAB5\\\\data\\\\image_5.jpg\",\n",
    "    \"C:\\\\Users\\\\ragsh\\\\Desktop\\\\FALL 24\\\\RSN\\\\LAB5\\\\data\\\\image_6.jpg\"\n",
    "]\n",
    "images = [cv2.imread(img_path) for img_path in image_paths]\n",
    "\n",
    "# Create the panorama\n",
    "panorama = create_panorama(images)\n",
    "\n",
    "# Display the result\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(cv2.cvtColor(panorama, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
