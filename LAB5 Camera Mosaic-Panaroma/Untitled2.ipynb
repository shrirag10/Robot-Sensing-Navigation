{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d0bd9e1-1b5f-4c16-9481-bdca4e5ceef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient w.r.t. weights_hidden_layer1: [-6.89989701e-08 -9.65985581e-08 -5.51991761e-08 -4.13993821e-08\n",
      " -2.75995880e-08]\n",
      "Gradient w.r.t. weights_hidden_layer2: [1.37997862e-07 1.93197007e-07 1.10398290e-07 8.27987175e-08\n",
      " 5.51991450e-08]\n",
      "Gradient w.r.t. weights_output_layer: [array([4.13993727e-08, 1.37997909e-07]), array([0., 0.]), array([-9.32587334e-15, -3.10862445e-14])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = np.array([5, 7, 4, 3, 2])\n",
    "weight1 = np.array([[0, 2, -4, -1, 3], [-5, 4, 0, 1, -2]])  # Adjusted shape to (2, 5)\n",
    "weight2 = np.array([2, 1, -4, 3, 0])\n",
    "weightv1 = np.array([-1, 2])\n",
    "weightv2 = np.array([2, 4])\n",
    "weightv3 = np.array([1, 3])\n",
    "label = np.array([0, 0, 1])\n",
    "\n",
    "# Forward pass\n",
    "hidden_layer1 = np.dot(weight1, inputs)\n",
    "hidden_layer2 = np.dot(weight2, inputs)\n",
    "\n",
    "# Aggregate hidden layers to obtain scalar outputs\n",
    "hidden_output_1 = np.maximum(0, hidden_layer1).sum()  # ReLU and sum\n",
    "hidden_output_2 = max(0, hidden_layer2)  # ReLU on scalar\n",
    "final_hidden = np.array([hidden_output_1, hidden_output_2])\n",
    "\n",
    "output_1 = np.dot(weightv1, final_hidden)\n",
    "output_2 = np.dot(weightv2, final_hidden)\n",
    "output_3 = np.dot(weightv3, final_hidden)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "sigmoid_1 = sigmoid(output_1)\n",
    "sigmoid_2 = sigmoid(output_2)\n",
    "sigmoid_3 = sigmoid(output_3)\n",
    "final_sigmoid = np.array([sigmoid_1, sigmoid_2, sigmoid_3])\n",
    "\n",
    "exp_outputs = np.exp(final_sigmoid)\n",
    "softmax = exp_outputs / np.sum(exp_outputs)\n",
    "\n",
    "grad_loss = softmax - label\n",
    "\n",
    "sigmoid_out1 = sigmoid_1 * (1 - sigmoid_1)\n",
    "sigmoid_out2 = sigmoid_2 * (1 - sigmoid_2)\n",
    "sigmoid_out3 = sigmoid_3 * (1 - sigmoid_3)\n",
    "\n",
    "grad_loss1 = grad_loss[0] * sigmoid_out1\n",
    "grad_loss2 = grad_loss[1] * sigmoid_out2\n",
    "grad_loss3 = grad_loss[2] * sigmoid_out3\n",
    "\n",
    "grad_weights_output_1 = grad_loss1 * final_hidden\n",
    "grad_weights_output_2 = grad_loss2 * final_hidden\n",
    "grad_weights_output_3 = grad_loss3 * final_hidden\n",
    "\n",
    "grad_hidden_layer = (\n",
    "    grad_loss1 * weightv1 +\n",
    "    grad_loss2 * weightv2 +\n",
    "    grad_loss3 * weightv3\n",
    ")\n",
    "\n",
    "gradinput_1 = grad_hidden_layer[0] * (1 if hidden_output_1 > 0 else 0)\n",
    "gradinput_2 = grad_hidden_layer[1] * (1 if hidden_output_2 > 0 else 0)\n",
    "\n",
    "grad_weights_hidden_layer1 = gradinput_1 * inputs\n",
    "grad_weights_hidden_layer2 = gradinput_2 * inputs\n",
    "\n",
    "print(\"Gradient w.r.t. weights_hidden_layer1:\", grad_weights_hidden_layer1)\n",
    "print(\"Gradient w.r.t. weights_hidden_layer2:\", grad_weights_hidden_layer2)\n",
    "print(\"Gradient w.r.t. weights_output_layer:\", [grad_weights_output_1, grad_weights_output_2, grad_weights_output_3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518832e-9f72-4b8b-ba8d-02e2cd9cb7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
